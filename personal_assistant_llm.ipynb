{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFhmIYxTiIPC+oQ9zbKVcE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SlayerDraco/personal-assistant-LLM/blob/main/personal_assistant_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "NSD02SDmGely"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets tokenizers accelerate deepspeed\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Create your structure\n",
        "folders = [\n",
        "    \"config\", \"tokenizer\", \"data/raw\", \"data/processed\",\n",
        "    \"training\", \"model\", \"inference\"\n",
        "]\n",
        "for folder in folders:\n",
        "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úîÔ∏è Environment setup complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXZKhdMvDG2z",
        "outputId": "b560cda6-0d28-4786-a3e8-9acb903a5181"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Environment setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the tokeniser\n"
      ],
      "metadata": {
        "id": "3MEXhYrbjmHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Path(\"data/raw\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "sample_text = \"\"\"\n",
        "User: Set a reminder every hour to drink water.\n",
        "AI: Got it. Hourly hydration threats activated.\n",
        "\n",
        "User: Take a note that I need to submit the assignment tomorrow.\n",
        "AI: Noted. Should I also remind you five times before the deadline or let you panic naturally?\n",
        "\n",
        "User: Schedule a call with Mom at 8 PM.\n",
        "AI: Scheduled. Tell her I said hi and that you're still alive.\n",
        "\n",
        "User: What's on my agenda today?\n",
        "AI: Cry a little, do some work, pretend you're fine. Oh, and a meeting at 3 PM.\n",
        "\n",
        "User: Draft a message for my professor about the late submission.\n",
        "AI: \"Dear Sir, I deeply regret being a walking deadline disaster...\" ‚Äî want me to send that?\n",
        "\n",
        "User: Tell me a joke.\n",
        "AI: You. Trying to wake up before 10 AM.\n",
        "\n",
        "User: Delete my last note.\n",
        "AI: Gone. Like your motivation.\n",
        "\n",
        "User: How's the weather?\n",
        "AI: Perfect for staying in and questioning your life choices.\n",
        "\n",
        "User: Play some music.\n",
        "AI: Playing your favorite: Lo-fi beats to cry and code to.\n",
        "\n",
        "User: Set an alarm for 7 AM.\n",
        "AI: Set. But we both know you're hitting snooze.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"data/raw/sample_conversations.txt\", \"w\") as f:\n",
        "    f.write(sample_text.strip())\n",
        "\n",
        "print(\"‚úÖ Sample dataset created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBihXC70w_x3",
        "outputId": "d2aa4700-1fa9-4fd6-fa3b-8edfab151d71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Sample dataset created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "from pathlib import Path\n",
        "\n",
        "# Load the pre-trained GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Add your custom domain-specific tokens\n",
        "new_tokens = [\n",
        "    \"<remind>\", \"<snooze>\", \"<hydrate>\", \"<assistant>\", \"<user>\",\n",
        "    \"<sarcasm>\", \"<panic_mode>\", \"<cry>\", \"<procrastinate>\"\n",
        "]\n",
        "tokenizer.add_tokens(new_tokens)\n",
        "\n",
        "print(f\"‚úÖ Added {len(new_tokens)} new tokens. New vocab size: {len(tokenizer)}\")\n",
        "\n",
        "# Save this upgraded tokenizer\n",
        "Path(\"tokenizer\").mkdir(parents=True, exist_ok=True)\n",
        "tokenizer.save_pretrained(\"tokenizer/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13rrS8Nmj_qw",
        "outputId": "4db3b183-0c70-4c9e-e1dc-8343b4fdbad0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Added 9 new tokens. New vocab size: 50266\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tokenizer/tokenizer_config.json',\n",
              " 'tokenizer/special_tokens_map.json',\n",
              " 'tokenizer/vocab.json',\n",
              " 'tokenizer/merges.txt',\n",
              " 'tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk wordfreq lemminflect\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "import lemminflect\n",
        "from lemminflect import getAllInflections\n",
        "from wordfreq import top_n_list\n",
        "import random\n",
        "\n",
        "all_words = set()\n",
        "\n",
        "# 1. WordNet base words and inflections\n",
        "for synset in wn.all_synsets():\n",
        "    for lemma in synset.lemmas():\n",
        "        word = lemma.name().lower().replace('_', ' ')\n",
        "        all_words.add(word)\n",
        "\n",
        "        # Derivational forms\n",
        "        if lemma.derivationally_related_forms():\n",
        "            for related in lemma.derivationally_related_forms():\n",
        "                all_words.add(related.name().lower().replace('_', ' '))\n",
        "\n",
        "# 2. Lemminflect inflections\n",
        "print(\"Expanding via lemminflect...\")\n",
        "temp_words = list(all_words)\n",
        "for word in temp_words:\n",
        "    try:\n",
        "        inflections = getAllInflections(word)\n",
        "        for form_list in inflections.values():\n",
        "            for form in form_list:\n",
        "                all_words.add(form.lower())\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# 3. Add high-frequency words from wordfreq (English)\n",
        "print(\"Adding top words from wordfreq...\")\n",
        "top_words = top_n_list('en', 10000)\n",
        "all_words.update(top_words)\n",
        "\n",
        "print(f\"‚úÖ Total unique words generated: {len(all_words)}\")\n",
        "\n",
        "# Optional: Save to a .txt file for tokenizer training\n",
        "with open(\"mew.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(sorted(all_words)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK6k_ngMJdmv",
        "outputId": "e17591b5-8b79-4847-faf4-c76eafe5c087"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: lemminflect in /usr/local/lib/python3.11/dist-packages (0.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (6.3.1)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (3.5.0)\n",
            "Requirement already satisfied: locate<2.0.0,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (1.1.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lemminflect) (2.0.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes>=3.0->wordfreq) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from marisa-trie>=1.1.0->language-data>=1.2->langcodes>=3.0->wordfreq) (75.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expanding via lemminflect...\n",
            "Adding top words from wordfreq...\n",
            "‚úÖ Total unique words generated: 176210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load your upgraded tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"tokenizer/\")\n",
        "\n",
        "# Load your data\n",
        "with open(\"mew.txt\", \"r\") as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# Encode the data\n",
        "encoded = tokenizer.encode(text_data, add_special_tokens=True)\n",
        "\n",
        "# Save the encoded data\n",
        "with open(\"data/processed/encoded_data.txt\", \"w\") as f:\n",
        "    f.write(\" \".join(map(str, encoded)))\n",
        "\n",
        "print(f\"‚úÖ Encoded {len(encoded)} tokens and saved to `data/processed/encoded_data.txt`\")\n"
      ],
      "metadata": {
        "id": "l8S_yLvtMOZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datasets import Dataset\n",
        "\n",
        "# Paths\n",
        "encoded_path = \"data/processed/encoded_data.txt\"\n",
        "npy_path = \"data/processed/encoded_data.npy\"\n",
        "hf_dataset_path = \"data/processed/hf_tokenized_dataset\"\n",
        "shard_prefix = \"data/processed/shard_\"\n",
        "num_shards = 4  # You can increase this based on your compute\n",
        "\n",
        "# Step 1: Load token IDs\n",
        "print(\"üîÑ Loading token IDs from text...\")\n",
        "with open(encoded_path, \"r\") as f:\n",
        "    token_ids = [int(line.strip()) for line in f.readlines() if line.strip().isdigit()]\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(token_ids):,} tokens.\")\n",
        "\n",
        "# Step 2: Save as .npy (binary)\n",
        "print(\"üíæ Saving token IDs as .npy...\")\n",
        "np_array = np.array(token_ids, dtype=np.uint16)  # Use uint32 if vocab is huge\n",
        "np.save(npy_path, np_array)\n",
        "print(f\"‚úÖ Saved to {npy_path} with shape {np_array.shape}.\")\n",
        "\n",
        "# Step 3: Shard into smaller .npy files\n",
        "print(f\"üî™ Sharding into {num_shards} pieces...\")\n",
        "shard_size = len(np_array) // num_shards\n",
        "for i in range(num_shards):\n",
        "    start = i * shard_size\n",
        "    end = (i + 1) * shard_size if i < num_shards - 1 else len(np_array)\n",
        "    shard = np_array[start:end]\n",
        "    np.save(f\"{shard_prefix}{i}.npy\", shard)\n",
        "    print(f\"   üß© Shard {i} saved: {len(shard)} tokens.\")\n",
        "\n",
        "# Step 4: Convert to HuggingFace dataset\n",
        "print(\"üß† Converting to HuggingFace Dataset format...\")\n",
        "dataset = Dataset.from_dict({\"input_ids\": token_ids})\n",
        "dataset.save_to_disk(hf_dataset_path)\n",
        "print(f\"‚úÖ Saved HuggingFace dataset to {hf_dataset_path}\")\n"
      ],
      "metadata": {
        "id": "DopzPrjolJhl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}